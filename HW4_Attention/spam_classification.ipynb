{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5UtdbaBpE3YT"
      },
      "source": [
        "**Name: Hamidreza Amirzadeh**\n",
        "\n",
        "**Std. No.: 401206999**\n",
        "\n",
        "<div style=\"direction:rtl;line-height:200%;\"><font face=\"B Nazanin\" size=5>\n",
        "<p>\n",
        "توجه: این تمرین با همفکری دانشجو آقای امیرمحمد منصوریان انجام شده است.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XrtOBu1RD_"
      },
      "source": [
        "# 0. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtC8ShqGz19c"
      },
      "source": [
        "In this notebook, we aim to make a classifier to identify spam messages. We will use a dataset that is consisted of 5000 SMS texts. Some of theses texts are labeled as `spam` while the rest are considered `ham`.\n",
        "\n",
        "For this aim, we will use **BERT** word-embeddings from the `transformers` library. We will not train a transformer, as it requires a lot of GPU power, but we will fine-tune a pre-trained transformer encoder (**BERT**) for our classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mUv6qR8uoGB0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Keyring is skipped due to an exception: 'keyring.backends'\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
            "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "# !pip install --quiet transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RDKTDFwm1oSY"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur81S9OI1X4D"
      },
      "source": [
        "# 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8a0ay6ADwhBT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "df = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7SXFPoDdx1jP"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gcVqsZZT8eaK"
      },
      "outputs": [],
      "source": [
        "######################   TODO 1.1   ########################\n",
        "# change the label column so that `spam` labels get `1` \n",
        "# and `ham` gets `0`\n",
        "df['label'].replace({'spam':1, 'ham':0}, inplace=True)\n",
        "###################### (2 points) ##########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HC6Ll3VS9iJo"
      },
      "outputs": [],
      "source": [
        "######################   TODO 1.2   ########################\n",
        "# split the dataframe into two sections of train and val. \n",
        "# keep the train size 10 times of val.\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(df, train_size=0.9, random_state=10)\n",
        "###################### (3 points) ##########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9-B7gl1O79Wx"
      },
      "outputs": [],
      "source": [
        "######################   TODO 1.3   ########################\n",
        "# based on what you did in homework 1, create a dataset and \n",
        "# a dataloader. Your dataset should return a text with its \n",
        "# respective label when iterated.\n",
        "###################### (10 points) ##########################\n",
        "\n",
        "class CustomDataset:\n",
        "    def __init__(self, df):\n",
        "        self.dataset = df\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset['text'][index], self.dataset['label'][index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "class CustomDataloader:\n",
        "    def __init__(self, dataset, batch_size, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "    def __len__(self):\n",
        "        return ceil(len(self.dataset)/self.batch_size)\n",
        "\n",
        "    def __iter__(self, calm=True):\n",
        "        indexes = list(range(len(self.dataset)))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "        for idx in range(0, len(self.dataset), self.batch_size):\n",
        "            batch_texts = []\n",
        "            batch_labels = []\n",
        "            \n",
        "            batch_indexes = list(range(idx, min(idx+self.batch_size, len(self.dataset))))\n",
        "            batch_indexes = [indexes[i] for i in batch_indexes]\n",
        "\n",
        "            for i in batch_indexes:\n",
        "                lbl,txt=self.dataset.dataset.iloc[indexes[i]]\n",
        "                batch_texts.append(txt)\n",
        "                batch_labels.append(lbl)\n",
        "\n",
        "            yield batch_texts, batch_labels\n",
        "        return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lFj6H3L19yYh"
      },
      "outputs": [],
      "source": [
        "######################   TODO 1.4   ########################\n",
        "# initialize a dataloader for each of your train and val\n",
        "# splits.\n",
        "train_dataset = CustomDataset(df_train)\n",
        "val_dataset = CustomDataset(df_val)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataloader = CustomDataloader(train_dataset, batch_size)\n",
        "val_dataloader = CustomDataloader(val_dataset, batch_size)\n",
        "###################### (5 points) ##########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onghqm9A-DlN"
      },
      "source": [
        "# 2. Pretrained Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSHF9OK-H7t"
      },
      "source": [
        "In this section we will use the pretrained **BERT** model from the `transformers` library with its respective `tokenizer`. **BERT** is a transformer encoder which is suited for various downstream NLP tasks namely *Sequence classification*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wYJGf9Thu2Cp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Defining the tokenizer and model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Gi7zqd_WybLR"
      },
      "outputs": [],
      "source": [
        "text = \"What is your name?\"\n",
        "tokenized = bert_tokenizer(text, max_length=128, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "encoding = bert_model(**tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cO5AxMY_jut"
      },
      "source": [
        "**TODO 2.1.** In section bellow, try to explain the arguments that `bert_tokenizer` gets as input. (text, max_length, padding, truncation, return_tensors) *(10 points)*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kBbLb7UG_8Oj"
      },
      "source": [
        "**text**: the string we want to be tokenized\n",
        "\n",
        "**max_length**: specified maximum length of tokenized text\n",
        "\n",
        "**padding**: It can be set to the 'longest' to pad to the longest sequence in the batch, or 'max_length' to pad to a length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided, or False or 'do_not_pad' to not pad the sequences.\n",
        "\n",
        "**truncation**: It can be set to the 'only_first' to only truncate the first sentence of a pair to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None), or 'only_second' to only truncate the second sentence of a pair to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided, or 'longest_first' which will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached, or False or 'do_not_truncate' to not truncate the sequences.\n",
        "\n",
        "**return_tensors**: Specifies the datatype returned. pt stands for pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch8y4w-yBskU"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqktej-9B39V"
      },
      "source": [
        "If you inspect the `encoding` of the `BERT`, you will realize that `BERT` gives a vector for each of the tokens included in the input sentence. However, all of these word tokens are not needed for a simple classification task.\n",
        "\n",
        "Instead, we can use the first token representation, as it captures the whole tokens meanings. `BERT` provides this token for us in a special variable called `pooler_output`. We will use this `pooler_output` as the input of our classification head inside our classifier model.\n",
        "![BERT pooler output](https://miro.medium.com/max/1100/1*Or3YV9sGX7W8QGF83es3gg.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "65p2L9_zyjHW"
      },
      "outputs": [],
      "source": [
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, embedding_tokenizer, embedding_model):\n",
        "        super().__init__()\n",
        "        ######################   TODO 3.1   ########################\n",
        "        # construct layers and structure of the network\n",
        "        self.embedding_size = 768\n",
        "\n",
        "        self.tokenizer = embedding_tokenizer\n",
        "        self.embedding = embedding_model\n",
        "        self.classifier = torch.nn.Linear(self.embedding_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        ###################### (10 points) #########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ######################   TODO 3.2   ########################\n",
        "        # implement the forward pass of your model. first tokenizer\n",
        "        # the sentence, the get the embeddings from your language\n",
        "        # model, then use the `pooler_output` for your classifier\n",
        "        # layer. \n",
        "        tokenized = self.tokenizer(x, max_length=128, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "        encoding = self.embedding(**tokenized)\n",
        "        return self.sigmoid(self.classifier(encoding.pooler_output))\n",
        "        ###################### (10 points) #########################\n",
        "\n",
        "    def predict(self, x):\n",
        "        ######################   TODO 3.3   ########################\n",
        "        # get the predicted class of x.\n",
        "        if self.forward(x).item() > 0.5: \n",
        "            return 1\n",
        "        else: \n",
        "            return 0\n",
        "        ###################### (5 points) #########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubb_GkQBBwId"
      },
      "source": [
        "# 4. Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OcMj49f8686B"
      },
      "outputs": [],
      "source": [
        "######################   TODO 4.1   ########################\n",
        "# define the learning parameters here (lr and epochs.)\n",
        "# then initilizer your model, an appropriate optimizer\n",
        "# and loss function.\n",
        "model = SpamClassifier(bert_tokenizer, bert_model)\n",
        "lr, epochs = 10e-4, 10\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "###################### (10 points) ##########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, \n",
        "        train_dataloader, val_dataloader, model,\n",
        "        optimizer, criterion, *args, **kwargs\n",
        "    ):\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.model = model\n",
        "        self.best_model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_loss = None\n",
        "        self.min_val_loss = np.inf\n",
        "\n",
        "    def train(self, epochs, log_each_n_percent_epoch):\n",
        "        train_steps = len(self.train_dataloader)\n",
        "        log_steps = int(train_steps * log_each_n_percent_epoch/100)\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"epoch {epoch+1} started\".title().center(50, \"=\"))\n",
        "            train_loss = 0.0\n",
        "            listLoss = []\n",
        "            for step, (data, labels) in enumerate(self.train_dataloader):\n",
        "                ######################   TODO 3.1   ########################\n",
        "\n",
        "                labels_pred = self.model(data)\n",
        "                labels = torch.tensor(list(map(int, labels))).view(len(labels),-1)\n",
        "                loss = self.criterion(labels_pred, labels)\n",
        "                listLoss.append(loss.item())\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                ###################### (10 points) #########################\n",
        "\n",
        "                if step % log_steps == 1:\n",
        "                    self.val_loss, accuracy = self.evaluate(save=True)\n",
        "                    self.val_losses.append(self.val_loss)\n",
        "                    self.train_losses.append(np.sum(listLoss)/len(listLoss))\n",
        "                    listLoss = []\n",
        "                    info_text = f'Validation Loss: {self.val_loss:.6f}\\t Accuracy-score: {accuracy:.2f}'\n",
        "                    print(info_text)\n",
        "                    self.post_evaluation_actions()\n",
        "                    \n",
        "\n",
        "    def evaluate(self, save=False):\n",
        "        listLoss =[]\n",
        "        accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            y_true, y_pred = [], []\n",
        "            ######################   TODO 3.2   ########################\n",
        "\n",
        "            for step, (data, labels) in enumerate(self.val_dataloader):\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                labels_pred = self.model(data)\n",
        "                labels = torch.tensor(list(map(int, labels))).view(len(labels),-1)\n",
        "                loss = self.criterion(labels_pred, labels)\n",
        "                labels_pred_index = self.model.predict(data)\n",
        "                listLoss.append(loss.item())\n",
        "                list_Y = labels.tolist()\n",
        "                list_Predict_Y = labels_pred_index.tolist()\n",
        "                for i in range(len(list_Y)):\n",
        "                    y_true.append(list_Y[i])\n",
        "                    y_pred.append(list_Predict_Y[i])\n",
        "                \n",
        "            val_loss = np.sum(listLoss)\n",
        "            joint = list(zip(y_true, y_pred))\n",
        "            accuracy = np.mean([l1==l2 for (l1,l2) in joint])\n",
        "            \n",
        "            ###################### (5 points) #########################\n",
        "            self.val_losses.append(val_loss)\n",
        "            return val_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(train_dataloader, val_dataloader, model, optimizer, criterion)\n",
        "trainer.train(epochs=5, log_each_n_percent_epoch=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "518uVqVe7Aly"
      },
      "outputs": [],
      "source": [
        "######################   TODO 4.2   ########################\n",
        "# implement your training loop and train your model.\n",
        "# return to homework 1 if needed.\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, train_dataloader, val_dataloader, model, optimizer, criterion):\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.model = model\n",
        "        self.best_model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.model.train()\n",
        "        train_steps = len(self.train_dataloader.dataset.dataset)\n",
        "        min_val_loss = np.inf\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"epoch {epoch+1} started\".title().center(50, \"=\"))\n",
        "            train_loss = 0.0\n",
        "            for step, (data, labels) in enumerate(self.train_dataloader):\n",
        "                self.optimizer.zero_grad()\n",
        "                prediction = self.model(data)\n",
        "                labels = torch.tensor(list(map(int, labels)))\n",
        "                loss = self.criterion(prediction, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "        self.test()\n",
        "    \n",
        "    def test(self, save=False):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_true, y_pred = [], []\n",
        "            val_loss = 0.0\n",
        "            for data, labels in self.val_dataloader:\n",
        "\n",
        "                prediction = self.model(data)\n",
        "                loss = self.criterion(prediction, labels)\n",
        "                val_loss += loss.item() * len(data)\n",
        "\n",
        "                prediction = self.model.predict(data)\n",
        "                y_pred.extend(prediction.cpu().numpy())\n",
        "\n",
        "                labels = labels.data.cpu().numpy()\n",
        "                y_true.extend(labels)\n",
        "\n",
        "            val_loss = val_loss / len(self.val_dataloader)\n",
        "            accuracy = accuracy_score(y_true, y_pred)\n",
        "            self.val_losses.append(val_loss)\n",
        "            return val_loss, accuracy\n",
        "\n",
        "\n",
        "trainer = Trainer(train_dataloader, val_dataloader, model, optimizer, criterion)\n",
        "trainer.train(epochs)\n",
        "###################### (10 points) ##########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmPsHMXUGILx"
      },
      "source": [
        "# 5. Using HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EFgSOE4GLXK"
      },
      "source": [
        "[HuggingFace library](http://huggingface.co/) has built a nice API for NLP tasks around the transformers. To get familiar with this comrehensive library, In this section you are asked to use the huggingface `Trainer`, `Dataset`, and `BertForSequenceClassification` to do what we did above again.\n",
        "\n",
        "Feel free to refer to the library documentation to learn about these modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP7RRbz5Fx16"
      },
      "outputs": [],
      "source": [
        "######################   TODO 5.1   ########################\n",
        "# use huggingface Trainer and Dataset API and train the \n",
        "# `SpamClassifier`. You should not use the `SpamClassifier`\n",
        "# we implemented previously. Instead you should use \n",
        "# `BertForSequenceClassification` here.\n",
        "###################### (25 points) #########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.target = df['label'].values\n",
        "        self.data = df['text'].values\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "      encoded_sent = self.tokenizer.encode_plus(\n",
        "            text=self.data[index],  \n",
        "            add_special_tokens=True,        \n",
        "            max_length=128,                  \n",
        "            pad_to_max_length=True,         \n",
        "            return_tensors='pt',           \n",
        "            return_attention_mask=True      \n",
        "            )\n",
        "      input_ids = encoded_sent.get('input_ids')\n",
        "      attn_masks = encoded_sent.get('attention_mask')\n",
        "      return {\n",
        "          'input_ids': input_ids.squeeze(0),\n",
        "          'attention_mask' : attn_masks.squeeze(0),\n",
        "          'labels': self.target[index]\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = Dataset(df_train, bert_tokenizer)\n",
        "test_dataset = Dataset(df_val, bert_tokenizer)\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "training_args = TrainingArguments( output_dir=\"DRIVE_PATH\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    seed=0,\n",
        "    load_best_model_at_end=True,)\n",
        "\n",
        "trainer = Trainer(model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d7ad8de7c781800bbc492cbb4adbbd955e1e5e99710384a9653638cde1a95298"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
